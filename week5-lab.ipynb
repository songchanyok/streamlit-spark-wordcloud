{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cf386b-3589-406a-9e32-b1d08916cdb5",
   "metadata": {},
   "source": [
    "## #다음 testfile1.txt, testfile2.txt, testfile3.txt에 대해서 질문에 답하시오. \n",
    "1. testfile1.txt 에 대해서 각 사람별 평균을 구하시오.\n",
    "2. testfile2.txt, testfile3.txt 에 공통적으로 나타나는 단어에 대해 빈도수를 내림차순으로 출력하시오. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "00ffb59b-31ed-4cf3-94cc-18fd213ae017",
   "metadata": {},
   "source": [
    "---- testfile1.txt ----\n",
    "파일설명: 이름, 중간 및 기말시험 점수\n",
    "\n",
    "seokil, 50\n",
    "seokil, 60\n",
    "gildong, 60\n",
    "gildong, 80\n",
    "-----------------------\n",
    "\n",
    "---- testfile2.txt ----\n",
    "pairRDD = sc.parallelize( [('panda', 0), ('pink', 3), ('pirate', 3), ('panda', 1), ('pink', 4)] )\n",
    "result1 = pairRDD.groupByKey()\n",
    "result2 = result1.map(lambda x : (x[0], len(x[1])))\n",
    "result3 = result1.map(lambda x : (x[0], list(x[1])))\n",
    "\n",
    "print(result2.collect())\n",
    "print(result3.collect())\n",
    "-----------------------\n",
    "\n",
    "---- testfile3.txt ----\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "pairRDD1 = sc.parallelize( [('panda', 0), ('pink', 3)])\n",
    "pairRDD2 = sc.parallelize( [('pirate', 3), ('panda', 1), ('pink', 4)] )\n",
    "result1 = pairRDD1.cogroup(pairRDD2)\n",
    "\n",
    "print(result1.collect())\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74f2ee4-b81b-4792-b9db-bb9aed4e8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.master(\"local\").appName(\"week5-lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2e3b1d-4b5b-47d8-b450-ecbc34a15ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"pairRDD = sc.parallelize( [('panda', 0), ('pink', 3), ('pirate', 3), ('panda', 1), ('pink', 4)] )\", 'result1 = pairRDD.groupByKey()', 'result2 = result1.map(lambda x : (x[0], len(x[1])))', 'result3 = result1.map(lambda x : (x[0], list(x[1])))', '']\n",
      "['sc = SparkContext(conf=conf)', '']\n"
     ]
    }
   ],
   "source": [
    "tf2 = spark_session.sparkContext.textFile(\"./data/testfile2.txt\")\n",
    "tf3 = spark_session.sparkContext.textFile(\"./data/testfile3.txt\")\n",
    "print(tf2.take(5))\n",
    "print(tf3.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b71f08d-5a22-4529-868d-a1f230c8f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pairrdd', 2), ('sc', 1), ('parallelize', 1), ('panda', 2), ('pink', 2), ('pirate', 1), ('result1', 3), ('groupbykey', 1), ('result2', 2), ('map', 2)]\n"
     ]
    }
   ],
   "source": [
    "import sys, re\n",
    "\n",
    "wordcounts = tf2.filter(lambda line: len(line) > 0) \\\n",
    "\t\t\t   .flatMap(lambda line: re.split('\\W+', line)) \\\n",
    "\t\t\t   .filter(lambda word: len(word) > 1) \\\n",
    "                .map(lambda word:(word.lower(),1)) \\\n",
    "\t\t\t   .reduceByKey(lambda v1, v2: v1 + v2) \\\n",
    "\t\t\t   .persist()\n",
    "\n",
    "                \n",
    "print(wordcounts.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc3e1c6b-bc38-4af4-b196-045765d0ee1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sc', 3),\n",
       " ('sparkcontext', 1),\n",
       " ('conf', 2),\n",
       " ('pairrdd1', 2),\n",
       " ('parallelize', 2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3rdd_wordcnt = tf3.filter(lambda line: len(line) > 0) \\\n",
    "\t\t\t   .flatMap(lambda line: re.split('\\W+', line)) \\\n",
    "\t\t\t   .filter(lambda word: len(word) > 1) \\\n",
    "\t\t\t   .map(lambda word:(word.lower(),1)) \\\n",
    "\t\t\t   .reduceByKey(lambda v1, v2: v1 + v2) \\\n",
    "\t\t\t   .persist()\n",
    "t3rdd_wordcnt.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "832ea27b-81ca-43bb-8b90-3ab5e0300807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('result1', 5), ('sc', 4), ('panda', 4), ('pink', 4), ('parallelize', 3), ('print', 3), ('collect', 3), ('pirate', 2)]\n"
     ]
    }
   ],
   "source": [
    "common=wordcounts.join(t3rdd_wordcnt).mapValues(lambda x: x[0]+x[1]).sortBy(lambda x: x[1], ascending=False)\n",
    "print(common.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d19a2-91fb-41ad-acdf-9ea7d56249a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
